---
title: "Exploração dos Clusters"
output:
  html_document:
    df_print: paged
date: "Outubro de 2019"
---

```{r setup_evaluation, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos =  "h")
knitr::opts_knit$set(root.dir = "./")

# data exploration libraries
library(tibble)
library(dplyr)
library(tidyr)
library(stringr)
library(lubridate)
library(knitr)

# clusters analysis libraries
library(fpc)
library(factoextra)
library(cluster)
library(ggcorrplot)
library(reshape2)

# plotting libraries
library(ggplot2)
library(plotly)
library(ggthemes)
```

```{r scripts_evaluation, include=FALSE}
# loading required steps before performing the analysis
source("../src/util/auxiliary_functions.R")
```

# Carregando a base de dados processada

Carregamos a base de dados alvo, previamente tratada durante os passos explicados na fase de **Data Preparation** (preparação de dados). 

```{r data_load, echo=TRUE}
target_data <- readRDS('../data/processed/target_dataset.rds')
```

Em seguida selecionamos apenas as variáveis necessárias para o processo de clusterização.

```{r data_subset, echo=TRUE}

# subsetting the dataset
target_data <- select(target_data, -c(idade, sexo, id))


```

## Checando a necessidade de padronização das variáveis

Um procedimento necessário para datasets que contém variáveis em diferentes escalas é a padronização e normalização das escalas para o correto cálculo das métricas de distâncias utilizadas pelos algoritmos.

Porém para este conjunto de dados específico todas as variáveis estão na mesma escala, sendo assim não é necessária a padronização das vaiáveis.

Obteremos os mesmos resultados com as variáveis padronizadas ou não.

```{r data_scale, eval = FALSE}

# target_data <- sapply(target_data, scale)

```


*******************************************************************************

# Métodos de Clusterização

## AGNES

O método AGNES (Agglomerative Nesting) é um dos tipos mais comums de métyodos de cluster hierárquico usado para agrupar objetos em clusters com base em sua similaridade. 

O algoritmo começa tratando cada observação como um cluster. Em seguida, pares de clusters são mesclados sucessivamente até que todos os clusters tenham sido mesclados em um grande cluster contendo todos os objetos. 

O resultado é uma representação baseada em árvore dos objetos, chamada dendrograma.

Primeiramente iniciamos calculando a matriz de distancias das observações de nossa amostra

```{r generating_AGNES_diss, echo=TRUE}

# compute the dissimilarity matrix
cluster_dist <- dist(target_data, method = "euclidean")

# inspecting a samples of the dissimilarity matrix
as.matrix(cluster_dist)[1:6, 1:6]
```

### AGNES com o método **ward**

Com a matriz de distâncias calculadas no passo assima aplicamos o algoritimo AGNES com o método **ward** para obter o modelo de clusterização.

```{r generating_AGNES_model_with_ward, echo=TRUE}

# applying the AGNES algorithm
cluster_agnes_ward <- agnes(cluster_dist, 
                       diss = TRUE, 
                       metric = 'euclidian', 
                       method = 'ward')

cluster_agnes_ward

```

Visualizando o dendrograma resultante da aplicação do algoritimo.

```{r generating_AGNES_viz_ward, echo=TRUE, out.width='100%'}

fviz_dend(cluster_agnes_ward)

```

Analisando o dendrograma é possível verificar que 3 clusters parece ser a melhor escolha para o número de clusters utilizando o método **ward**, pois existe uma diferença significativa na altura das quebras das observações.

Abaixo visualizados o dendrograma para 3 clusters.

```{r generating_AGNES_viz_2_ward, echo=TRUE, out.width='100%'}

cluster_size <- 3

fviz_dend(cluster_agnes_ward, k = cluster_size)

```

Também podemos visualizar a classificação das observações utilizando as dois principais componentes da técnica de redução de dimensionalidade PCA (Principal Component Analysis).

```{r generating_AGNES_viz_3_ward, echo=TRUE, out.width='100%'}

fviz_cluster(list(data = target_data, 
                  cluster = cutree(cluster_agnes_ward, k = cluster_size)),  
             show.clust.cent = FALSE)

```

Finalmente podemos analisar todas as métricas do cluster com o método **ward** utilizando a função **cluster.stats**.

```{r generating_AGNES_stats_ward, echo=TRUE}

cluster.stats(cluster_dist, cutree(cluster_agnes_ward, k = cluster_size))

```


### AGNES com o método **average**

Com a matriz de distâncias calculadas no passo assima aplicamos o algoritimo AGNES com o método **average** para obter o modelo de clusterização.

```{r generating_AGNES_model_with_average, echo=TRUE}

# applying the AGNES algorithm
cluster_agnes_average <- agnes(cluster_dist, 
                       diss = TRUE, 
                       metric = 'euclidian', 
                       method = 'average')

cluster_agnes_average

```

Visualizando o dendrograma resultante da aplicação do algoritimo.

```{r generating_AGNES_viz_average, echo=TRUE, out.width='100%'}

fviz_dend(cluster_agnes_average)

```

Analisando o dendrograma para o número de clusters utilizando o método **average**, não fica muito evidente qual o melhor "corte" de número de clusters. Ainda assim, é possível notar que existe uma melhor distinção de grupos em 3 clusters.

Abaixo visualizados o dendrograma para 3 clusters.

```{r generating_AGNES_viz_2_average, echo=TRUE, out.width='100%'}

cluster_size <- 3

fviz_dend(cluster_agnes_average, k = cluster_size)

```

Também podemos visualizar a classificação das observações utilizando as dois principais componentes da técnica de redução de dimensionalidade PCA (Principal Component Analysis).

```{r generating_AGNES_viz_3_average, echo=TRUE, out.width='100%'}

fviz_cluster(list(data = target_data, 
                  cluster = cutree(cluster_agnes_average, k = cluster_size)),  
             show.clust.cent = FALSE)

```

Finalmente podemos analisar todas as métricas do cluster com o método **average** utilizando a função **cluster.stats**.

```{r generating_AGNES_stats_average, echo=TRUE}

cluster.stats(cluster_dist, cutree(cluster_agnes_average, k = cluster_size))

```


## K-Means

O método de classificação por K-means é um dos algoritmos de aprendizado de máquina não supervisionados mais simples e populares.

O algoritmo começa com um primeiro grupo de centroides selecionados aleatoriamente, que são usados como pontos de partida para cada cluster e, em seguida, executa cálculos iterativos para otimizar as posições dos centroides.

Para cada iteração o algoritmo calcula a métrica de distância selecionada entre cada observação e o centroide classificando cada observação ao cluster cujo centroide está mais próximo.

Ele interrompe a criação e otimização de clusters quando:

1. Os centroides se estabilizaram - não há alteração em seus valores porque o agrupamento foi bem-sucedido.

2. O número definido de iterações foi alcançado.

O parâmetro mais importante para o método de k-means é a quantidade de clusters que o algoritmo irá utilizar.

Para encontrar a melhor quantidade de clusters podemos observar o comportamento de duas métricas, withinss e betweenss, que medem as distâncias intra-cluster e inter-clusters de cada observação e centroide dos clusters.

A melhor quantidade de k é dada no ponto onde existe uma estabilização entre as métricas.

Iremos utilizar uma função customizada por nós para definir a melhor quantidade de clusters a utilizar, basicamente definimos o melhor k como sendo aquele onde existe a menor diferença absoluta entre as métricas de withinss e betweenss.

Mais detalhes sobre a função podem ser encontrados na sessão de preparação de dados desta pagina.

```{r generating_kmeans_best_k, echo=TRUE, out.width='100%'}

find_best_k_kmeans(target_data, k_limit = 29, nstart = 100)$plot

```

Em seguida aplicamos o algoritmo k-means utilizando a melhor quantidade de clusters retornada pela nossa função.

```{r generating_kmeans, echo=TRUE}

cluster_size <- find_best_k_kmeans(target_data, k_limit = 29, nstart = 100)$best_k

cluster_kmeans <- kmeans(target_data, cluster_size, nstart = 100)

cluster_kmeans

```

Assim como fizemos para o método AGNES podemos visualizar a classificação, para o método k-means, das observações utilizando as dois principais componentes da técnica de redução de dimensionalidade PCA (Principal Component Analysis).

Observamos que ambos os métodos classificaram as observações nos mesmos clusters.

```{r generating_kmeans_viz, echo=TRUE, out.width='100%'}

# Visualizing clusters

fviz_cluster(list(data = target_data, cluster = cluster_kmeans$cluster),
             show.clust.cent = T)

```

Abaixo as métricas gerais do método de classificação k-means aplicado a este dataset.

Observamos que como a ambos os métodos AGNES e k-means obtém exatamente as mesmas métricas pois ambos os métodos classificaram os dados nos mesmos clusters.

A única diferença são os números de identificação associados a cada cluster.


```{r generating_kmeans_stats, echo=TRUE}

cluster.stats(cluster_dist, cluster_kmeans$cluster)

```

Na sessão de conclusão responderemos todas as perguntas iniciais do trabalho.